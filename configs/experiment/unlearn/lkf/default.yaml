# @package _global_

defaults:
  - override /model: Llama-3.2-3B-Instruct
  - override /trainer: GradAscent
  - override /data: unlearn
  - override /data/datasets@data.forget: LKF_St_forget
  - override /data/datasets@data.retain: LKF_St_retain

model:
  model_args:
    pretrained_model_name_or_path: meta-llama/Llama-3.2-3B-Instruct

retain_logs_path: null

# eval:
#   tofu:
#     forget_split: ${forget_split}
#     retain_logs_path: ${retain_logs_path}
#     overwrite: true
    
data:
  anchor: forget
  forget:
    LKF_St_forget: 
      args:
        hf_args:
          name: default
  retain:
    LKF_St_retain:
      args:
        hf_args:
          name: default

trainer:
  args:
    warmup_epochs: 1.0 # custom parameter
    learning_rate: 8e-6
    weight_decay: 0.01
    num_train_epochs: 10
    # save_strategy: steps
    # save_steps: 0.5

task_name: ???